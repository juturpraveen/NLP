- Word meaning - The idea represented by the word
- Synonyms: similary meanining
- Hypernym: hierarchy of word
- Traditional NLP is up untill 2012. After 2012 neural networks started in NLP.
- Orthogonal vectors: Vectors that do not have any kind of similarity.
- Trying to find similar words using a word's list (like word net) or some sort of dictionary is not practical. Hence
we need to look at different way(s) such that representation of the word itself gives the idea or amount of similarity.
- Instead of using lists or dictionaries, learn the word meaning by the context of it. Context is the set of words that appear nearby.
- Using the contexts, a dense vector is built to represent word. These word vectors are also called 'Word embeddings' or
'Word representations'.
- In these vectors each value/cell is considered as one dimension. If a word is represented as a vector of 100 values like 
word =[v1, v2, v3......v100]. 

#### Word2vec
- Word2vec is a way of learning word vectors.
